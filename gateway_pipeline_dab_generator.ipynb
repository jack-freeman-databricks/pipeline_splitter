{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29127251-b906-41a8-8052-4a2574aa7ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abfcf0f9-c2bf-4146-9958-a41faaa42dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from databricks.sdk import WorkspaceClient\nimport os, re, yaml\nfrom math import ceil\nfrom collections import defaultdict, deque\n\ndef generate_pipeline_gateway_ymls(\n    metadata_df,\n    output_name: str = \"generated\",\n    destination_catalog_default: str = \"poc2_test_catalog\",\n    destination_schema_default: str = \"final_saas_db\",\n    source_type_default: str = \"SQLSERVER\",\n    pipeline_table_cap: int = 150,\n    gateway_table_cap: int = 1000,\n    large_table_threshold: int = 50_000_000,\n    cdc_applier_timeout_seconds: str = \"600\",\n    cluster_node_type_id: str = \"m5d.large\",\n    cluster_driver_node_type_id: str = \"c5a.8xlarge\",\n    cluster_num_workers: int = 1,\n    output_base_dir: str = \"resources\",\n    use_dabs_references: bool = True,\n    work_client: WorkspaceClient | None = None,\n    debug: bool = False,\n):\n    \"\"\"\n    Generate Databricks Asset Bundle YMLs (gateways & pipelines) from a DataFrame.\n    - The DataFrame must contain: server_name, connection_name, database_name, schema_name, table_name, row_count, priority_flag\n    - Creates two files under:\n        resources/gateways/gateway_<output_name>.yml\n        resources/pipelines/pipeline_<output_name>.yml\n\n    Args:\n        metadata_df: Spark DataFrame with required columns\n        output_name: Base name for generated YAML files\n        destination_catalog_default: Default destination catalog name (or DABS resource key if use_dabs_references=True)\n        destination_schema_default: Default destination schema name (or DABS resource key if use_dabs_references=True)\n        source_type_default: Default source type (e.g., SQLSERVER)\n        pipeline_table_cap: Max tables per pipeline\n        gateway_table_cap: Max tables per gateway\n        large_table_threshold: Row count threshold for large tables\n        cdc_applier_timeout_seconds: CDC applier timeout\n        cluster_node_type_id: Worker node type\n        cluster_driver_node_type_id: Driver node type\n        cluster_num_workers: Number of workers\n        output_base_dir: Base directory for output files\n        use_dabs_references: If True, generate DABS variable references for schemas/catalogs\n        work_client: Optional WorkspaceClient instance\n        debug: Enable debug output\n    \"\"\"\n\n    # --- Helpers ---\n    def slug(s: str) -> str:\n        return re.sub(r\"_+\", \"_\", re.sub(r\"[^a-z0-9_]+\", \"_\", (s or \"\").lower())).strip(\"_\")\n\n    def normalize_server_name(s: str) -> str:\n        s = re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n        return s.lower()\n\n    def write_yml(obj, path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            yaml.safe_dump(obj, f, sort_keys=False)\n    \n    def get_catalog_reference(catalog_name: str) -> str:\n        \"\"\"Generate catalog reference - either DABS variable or direct name\"\"\"\n        if use_dabs_references:\n            # Assume schemas.yml defines: resources.schemas.<schema_name>.catalog_name\n            return f\"${{resources.schemas.{destination_schema_default}.catalog_name}}\"\n        return catalog_name\n    \n    def get_schema_reference(schema_name: str) -> str:\n        \"\"\"Generate schema reference - either DABS variable or direct name\"\"\"\n        if use_dabs_references:\n            return f\"${{resources.schemas.{schema_name}.name}}\"\n        return schema_name\n\n    # --- Derive output file names ---\n    gateways_filename  = f\"gateway_{output_name}.yml\"\n    pipelines_filename = f\"pipeline_{output_name}.yml\"\n\n    gateways_path  = os.path.join(output_base_dir, \"gateways\",  gateways_filename)\n    pipelines_path = os.path.join(output_base_dir, \"pipelines\", pipelines_filename)\n\n    # Generate references for catalog and schema\n    catalog_ref = get_catalog_reference(destination_catalog_default)\n    schema_ref = get_schema_reference(destination_schema_default)\n\n    # --- Load metadata from DataFrame ---\n    df = metadata_df.select(\n        \"server_name\", \"connection_name\", \"database_name\",\n        \"schema_name\", \"table_name\", \"row_count\", \"priority_flag\"\n    )\n\n    rows = []\n    for r in df.collect():\n        server_key  = normalize_server_name(r[\"server_name\"])\n        server_slug = slug(server_key)\n        rows.append({\n            \"server_key\": server_key,\n            \"server_slug\": server_slug,\n            \"connection_name\": r[\"connection_name\"],\n            \"source_catalog\":  r[\"database_name\"],\n            \"source_schema\":   r[\"schema_name\"],\n            \"source_table\":    r[\"table_name\"],\n            \"row_count\":       int(r[\"row_count\"] or 0),\n            \"priority_flag\":   int(r[\"priority_flag\"] or 0),\n            \"destination_catalog\": catalog_ref,\n            \"destination_schema\":  schema_ref,\n            \"source_type\":         source_type_default,\n        })\n\n    if debug:\n        print(f\"üì¶ Loaded {len(rows)} rows from DataFrame\")\n        if use_dabs_references:\n            print(f\"üîó Using DABS references: catalog={catalog_ref}, schema={schema_ref}\")\n\n    # --- Initialize Databricks client ---\n    w = work_client or WorkspaceClient()\n    connection_cache = {c.name: c.connection_id for c in w.connections.list()}\n\n    def resolve_connection_id(name: str) -> str:\n        cid = connection_cache.get(name)\n        if not cid:\n            print(f\"‚ö†Ô∏è Warning: Connection '{name}' not found in workspace connections.\")\n            return \"00000000-0000-0000-0000-000000000000\"\n        return cid\n\n    # --- Group rows per server ---\n    by_server = defaultdict(list)\n    for r in rows:\n        by_server[r[\"server_key\"]].append(r)\n\n    server_gateways = {}\n    server_pipelines = {}\n\n    # --- Allocation logic ---\n    for server_key, items in by_server.items():\n        server_slug = items[0][\"server_slug\"]\n\n        priority_items = [t for t in items if t[\"priority_flag\"] == 1]\n        normal_items   = [t for t in items if t[\"priority_flag\"] != 1]\n\n        large_items = [t for t in normal_items if t[\"row_count\"] >= large_table_threshold]\n        small_items = [t for t in normal_items if t[\"row_count\"] <  large_table_threshold]\n\n        pipelines = []\n\n        # 1Ô∏è‚É£ Dedicated pipelines for priority tables\n        for t in priority_items:\n            pname = f\"final_{server_slug}_prio_{slug(t['source_table'])}\"\n            pipelines.append({\"name\": pname[:100], \"tables\": [t]})\n\n        # 2Ô∏è‚É£ Normal pipelines: distribute large tables evenly, fill small\n        normal_count   = len(normal_items)\n        base_pipelines = max(ceil(max(normal_count, 1) / pipeline_table_cap), len(large_items))\n        bins = [{\"name\": f\"final_{server_slug}_ingestion_{i+1}\", \"tables\": []} for i in range(base_pipelines or 1)]\n\n        for idx, t in enumerate(large_items):\n            bins[idx % len(bins)][\"tables\"].append(t)\n\n        def large_count(bin_): \n            return sum(1 for tt in bin_[\"tables\"] if tt[\"row_count\"] >= large_table_threshold)\n\n        small_q = deque(small_items)\n        while small_q:\n            bins.sort(key=lambda b: (len(b[\"tables\"]), large_count(b)))\n            b = bins[0]\n            if len(b[\"tables\"]) >= pipeline_table_cap:\n                bname = f\"final_{server_slug}_ingestion_{len(bins)+1}\"\n                bins.append({\"name\": bname, \"tables\": []})\n                continue\n            b[\"tables\"].append(small_q.popleft())\n\n        pipelines.extend([b for b in bins if b[\"tables\"]])\n\n        # 3Ô∏è‚É£ Gateways per server (‚â§ gateway_table_cap tables/gateway)\n        total_tables = sum(len(p[\"tables\"]) for p in pipelines)\n        num_gateways = max(1, ceil(total_tables / gateway_table_cap))\n\n        base_conn_name   = items[0][\"connection_name\"]\n        resolved_conn_id = resolve_connection_id(base_conn_name)\n\n        gateways = []\n        for gidx in range(num_gateways):\n            gname = f\"final_{server_slug}_gateway_{gidx+1}\"\n            gateways.append({\n                \"resource_key\": f\"pipeline_{gname}\",\n                \"name\": gname,\n                \"connection_name\": base_conn_name,\n                \"connection_id\":   resolved_conn_id,\n                \"storage_catalog\": catalog_ref,\n                \"storage_schema\":  schema_ref,\n                \"gateway_storage_name\": gname,\n                \"source_type\": source_type_default,\n                \"assigned_table_count\": 0,\n            })\n\n        # Assign pipelines evenly across gateways\n        pipelines.sort(key=lambda p: len(p[\"tables\"]), reverse=True)\n        for p in pipelines:\n            gateways.sort(key=lambda g: g[\"assigned_table_count\"])\n            g = gateways[0]\n            g[\"assigned_table_count\"] += len(p[\"tables\"])\n            p[\"gateway_ref\"] = g[\"resource_key\"]\n\n        server_gateways[server_key]  = gateways\n        server_pipelines[server_key] = pipelines\n\n        if debug:\n            print(f\"üñ•Ô∏è {server_key}: {len(pipelines)} pipelines, {len(gateways)} gateways\")\n\n    # --- Build YMLs ---\n    gateways_yml = {\"resources\": {\"pipelines\": {}}}\n    for _, gateways in server_gateways.items():\n        for g in gateways:\n            gateways_yml[\"resources\"][\"pipelines\"][g[\"resource_key\"]] = {\n                \"name\": g[\"name\"],\n                \"clusters\": [{\n                    \"node_type_id\": cluster_node_type_id,\n                    \"driver_node_type_id\": cluster_driver_node_type_id,\n                    \"num_workers\": cluster_num_workers,\n                }],\n                \"gateway_definition\": {\n                    \"connection_name\": g[\"connection_name\"],\n                    \"connection_id\":   g[\"connection_id\"],\n                    \"gateway_storage_catalog\": g[\"storage_catalog\"],\n                    \"gateway_storage_schema\":  g[\"storage_schema\"],\n                    \"gateway_storage_name\":    g[\"gateway_storage_name\"],\n                    \"source_type\": g[\"source_type\"],\n                },\n                \"target\": g[\"storage_schema\"],\n                \"continuous\": True,\n                \"catalog\": g[\"storage_catalog\"],\n            }\n\n    pipelines_yml = {\"resources\": {\"pipelines\": {}}}\n    for _, pipes in server_pipelines.items():\n        for p in pipes:\n            key = f\"pipeline_{slug(p['name'])}\"\n            objects = [{\n                \"table\": {\n                    \"source_catalog\": t[\"source_catalog\"],\n                    \"source_schema\":  t[\"source_schema\"],\n                    \"source_table\":   t[\"source_table\"],\n                    \"destination_catalog\": t[\"destination_catalog\"],\n                    \"destination_schema\":  t[\"destination_schema\"],\n                }\n            } for t in p[\"tables\"]]\n            first = p[\"tables\"][0]\n            pipelines_yml[\"resources\"][\"pipelines\"][key] = {\n                \"name\": p[\"name\"],\n                \"configuration\": {\n                    \"pipelines.cdcApplierFetchMetadataTimeoutSeconds\": cdc_applier_timeout_seconds\n                },\n                \"ingestion_definition\": {\n                    \"ingestion_gateway_id\": f\"${{resources.pipelines.{p['gateway_ref']}.id}}\",\n                    \"objects\": objects,\n                    \"source_type\": first[\"source_type\"],\n                },\n                \"target\": first[\"destination_schema\"],\n                \"catalog\": first[\"destination_catalog\"],\n            }\n\n    # --- Write .yml files ---\n    write_yml(gateways_yml,  gateways_path)\n    write_yml(pipelines_yml, pipelines_path)\n\n    # --- Print summary ---\n    summary = {\n        \"servers\": len(by_server),\n        \"gateways\": sum(len(g) for g in server_gateways.values()),\n        \"pipelines\": sum(len(p) for p in server_pipelines.values()),\n        \"tables\": sum(len(p2[\"tables\"]) for ps in server_pipelines.values() for p2 in ps),\n        \"paths\": {\"gateways\": gateways_path, \"pipelines\": pipelines_path},\n        \"use_dabs_references\": use_dabs_references,\n    }\n\n    print(\"\\n‚úÖ YAML generation complete:\")\n    print(f\"  üìÅ Gateway file created:  {gateways_path}\")\n    print(f\"  üìÅ Pipeline file created: {pipelines_path}\")\n    print(f\"  üñ•Ô∏è Servers processed:     {summary['servers']}\")\n    print(f\"  üß© Gateways created:      {summary['gateways']}\")\n    print(f\"  üîÑ Pipelines created:     {summary['pipelines']}\")\n    print(f\"  üìä Tables assigned:       {summary['tables']}\")\n    if use_dabs_references:\n        print(f\"  üîó Using DABS schema references\")\n    print()\n\n    return summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2e6c52-d355-467f-8e69-59c600a45dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Example 1: Load from Unity Catalog table with DABS schema references\nmetadata_df = spark.table(\"jack_demos.pipeline_split.synthetic_table_inventory_refactor\")\n\nresult = generate_pipeline_gateway_ymls(\n    metadata_df=metadata_df,\n    output_name=\"synthetic_table_inventory_refactor\",\n    destination_catalog_default=\"poc2_test_catalog\",\n    destination_schema_default=\"final_saas_db\",\n    source_type_default=\"SQLSERVER\",\n    pipeline_table_cap=150,\n    gateway_table_cap=1000,\n    large_table_threshold=50_000_000,\n    output_base_dir=\"resources\",\n    use_dabs_references=True,  # Use DABS schema references (default)\n    debug=True,\n)\n\ndisplay(result)\n\n# Example 2: Create DataFrame from other sources\n# You can now create the DataFrame from CSV, API, or any other source\n# df = spark.read.csv(\"path/to/metadata.csv\", header=True)\n# result = generate_pipeline_gateway_ymls(\n#     metadata_df=df, \n#     output_name=\"my_ingestion\",\n#     use_dabs_references=False,  # Use hardcoded catalog/schema names\n#     ...\n# )"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "gateway_pipeline_dab_generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}